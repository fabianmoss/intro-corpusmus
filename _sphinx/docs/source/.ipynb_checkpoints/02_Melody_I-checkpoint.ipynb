{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melodies in Folk Songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On Jupyter Hub, change the kernel to Python 3.7!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import music21 as m21\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tragen Sie hier bitte Ihren username ein:\n",
    "USERNAME = \"fmoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for jupyter hubs\n",
    "# %env QT_QPA_PLATFORM=offscreen \n",
    "# # new user, create music21 environment variables.\n",
    "# m21.environment.set('musicxmlPath', value='/usr/bin/mscore')\n",
    "# m21.environment.set('musescoreDirectPNGPath', value='/usr/bin/mscore')\n",
    "# m21.environment.set('graphicsPath', value=f'/home/{USERNAME}') # change accordingly for your own username!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *Essen Folksong Collection*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we work with a corpus of melodies, the *Essen Folksong Collection* (EFC). \n",
    "There are several ways to access this corpus, for example through the interface provided by the Center for Computer Assisted Research in the Humanities (CCARH) at Stanford University: http://essen.themefinder.org/ or via http://kern.ccarh.org/browse?l=essen.\n",
    "\n",
    "A more convenient way to work with the pieces is by using the Python library `music21`. This library was developed and is maintaned my Mike Cuthbert at the MIT and is the most popular library for the computational analysis of symbolic music (i.e. scores). You can find its documentation here: http://web.mit.edu/music21/\n",
    "\n",
    "However, using `music21` requires some training and getting used to its particular API (the way how to interact with its functions). We will not get into too many details here but rather showcase how it can be used for our purposes. \n",
    "\n",
    "The first thing we do is to load the entire EFC and store it in a variable named `corpora`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "corpora = m21.corpus.getComposer('essenFolksong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the variable `corpora` shows that it consists of a list of file paths. Using the `len()` function, we can find out how many corpora are stored in the variable `corpora`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly call the variable `corpora` to see what it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `corpora` is a list of file paths, each of which points to a corpus in this collection. Note that the location depends on the location where `music21` is installed. If you would do this on your own computer, you would see different paths. The file names at the end of the file paths indicate what they contain, e.g. `altdeu10.abc` contains old German folksongs, `boehme10.abc` contains Czech folksongs, and `han1.abc` contains Chinese folksongs. \n",
    "\n",
    "The `.abc` file ending refers to the ABC notation for encoding melodies. You find more information about the ABC encoding here: http://abcnotation.com/\n",
    "\n",
    "For example, a song could be encoded like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_song = \"\"\"\n",
    "X:1\n",
    "T:Speed the Plough\n",
    "M:4/4\n",
    "C:Trad.\n",
    "K:G\n",
    "|:GABc dedB|dedB dedB|c2ec B2dB|c2A2 A2BA|\n",
    "  GABc dedB|dedB dedB|c2ec B2dB|A2F2 G4:|\n",
    "|:g2gf gdBd|g2f2 e2d2|c2ec B2dB|c2A2 A2df|\n",
    "  g2gf g2Bd|g2f2 e2d2|c2ec B2dB|A2F2 G4:|\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tripple quotes (`\"\"\"`) surrounding the ABC notation are used by Python to store multi-line text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we already understand from this encoding? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`music21` can load this string and display a graphical output of the score. This is done by a **parser**. A parser is a program that reads a file and produces a structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_example_song = m21.converter.parse(example_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not need to give it the entire string again because we have already saved it in the `example_song` variable. The purpose of variables is that you can refer to them later in your code without explicitly needing to state its value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the variable `parsed_example_song` now, however, does not really help us here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_example_song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns a somewhat cryptic statement that says that the variable countains a `music21.stream.Score` object. Understanding the internal organization of `music21` goes beyond this class.\n",
    "For us, it is suffient to know that these objects have certain associated functions, called **methods**, that we can use on them. To look at the score of this example song, we use the method `.show()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_example_song.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voil√†, this is much better! Now, let us compare the score output to the ABC encoding of the song:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the ABC notation makes already more sense. `T:Speed the Ploug` stands for the title, `M:4/4` for the meter, and `K:G` for the key of the song. The [ABC documentation](http://abcnotation.com/blog/2010/01/31/how-to-understand-abc-the-basics/) tells us that `X:1` encodes just a reference number, in case multiple pieces are stored in the same file (as in our case in the variable `corpora`, remember?). And the lines at the bottom encode the proper melody, where the letters represent note names that are organized into bars with or without repetition signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`music21` even gives us the option to listen to the song if we path the `midi` argument to the `.show()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_example_song.show(\"midi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what happens if we try to parse one of the corpora in the EFC? We can select a specific corpus by its **index** in the list. Python starts counting at 0, so the first file in the list corresponds to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is just the first file path in the variable `corpora`. Let's try to parse it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_corpus = m21.converter.parse(corpora[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the new variable `first_corpus` shows a difference to the example song before; we don't have a `music21.stream.Score` object but a `music21.stream.Opus` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we would call the `.show()` method on `first_corpus`, we would see the scores of all pieces that are in this particular corpus. But we don't know how many these are. It there are only three songs, it would not be a problem, but if there were thousands of songs, it could take a very long time to parse and display them all. Fortunately, all pieces in the collection have the `X:n` line that we saw above, so that we can directly reference them. With which number would we have to replace `n` if we wanted to look at the 7tst piece? Remember that Python starts counting at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_corpus[70].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_corpus[70].show(\"midi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that we can select items from lists by **indexing** them, `list[i]`. We can get ranges of lists by using the `:` character. For example,\n",
    "`list[:10]` shows the first ten elements, `list[10:]` shows everything after the ninth element, and `list[3:6]` shows elements 3, 4, and 5 (not 6!) of the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at individual songs is interesting for music analysis but for that the computational approach is not really necessary. We could as easily do the same by just looking at a book of scores. The power of computational methods becomes clearer when we start comparing different songs, potentially in a large number.\n",
    "\n",
    "To facilitate this comparison, we will first load all songs in all corpora of the EFC into a single list, called `songs` (this might take a couple of minutes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = [s for i in range(len(corpora)) for s in m21.converter.parse(corpora[i]) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a bit complicated but all it does is to go through all corpora and extract all songs into a new list. The way we did it is called **list comprehension** in Python. It is not important if you don't understand this now but feel free to look it up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `len()` function again, we see how many songs we have in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the list `songs` to compare two different songs. Again, we load the 71st song of the first corpus and store it now in a variable `german_song`, and we load chinese song with index 6200 into the variable `chinese_song`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_song = songs[70]\n",
    "chinese_song = songs[6200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to display these songs now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_song.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_song.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_song.show(\"midi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of songs..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now go on to a computational analysis of these two and all the other songs. Specifically, we wil compare their **melodic profiles**. To make things a bit simpler, we will just look at the notes. \n",
    "\n",
    "A note can be easily represented as a pair of **pitch** (its height) and its **duration**. For example, the first note of the *Die plappernden Junggesellen* could be represented as `(D4, 1/4)`; it is a quarter note on the pitch D4 (the 4 indicates the octave in which the note is). \n",
    "\n",
    "Another way to represent the pitch of notes is using **MIDI numbers**. MIDI stands for *Musical Instrument Digital Interface* and was developed for the communication between different electronic instruments such as keyboards. In MIDI, each note is simply associated with a number:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MIDI Pitch](img/midi_pitch.png)\n",
    "*Image from https://www.audiolabs-erlangen.de/resources/MIR/FMP/C1/C1S2_MIDI.html.*\n",
    "\n",
    "We can see that D4 is associated with the number 62. The second note, the G4, is associated with 62+5=67 because G is five semitones above D. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to work with pieces in this way, we define a **function** that gives us a list of notes for each piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notelist(piece):\n",
    "    \"\"\"\n",
    "    This function takes a song as input and returns a list of (pitch, duration) pairs, \n",
    "    where the duration is given in quarter notes.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame([ (note.pitch.midi, note.quarterLength) for note in piece.flat.notes ], columns=[\"MIDI Pitch\", \"Duration\"])\n",
    "    df[\"Onset\"] = df[\"Duration\"].cumsum()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the duration of a note is given in quarter notes, i.e. a quarter note has a duration of 1, a half note has a duration of 2, and an eighth note has a duration of 0.5.\n",
    "\n",
    "Let's display the first phrase (the first eight notes) of the German song:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notelist(german_song)[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we added another column, \"Onset\". What does it represent?\n",
    "\n",
    "This allows us now to look at the **melodic profile** of a particular song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_melodic_profile(notelist, ax=None, c=None, mean=False, Z=False, sections=False, standardized=False):\n",
    "    \n",
    "    if ax == None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    if standardized:\n",
    "        x = notelist[\"Rel. Onset\"]\n",
    "        y = notelist[\"Rel. MIDI Pitch\"]\n",
    "    else:\n",
    "        x = notelist[\"Onset\"]\n",
    "        y = notelist[\"MIDI Pitch\"]\n",
    "    \n",
    "    ax.step(x,y, color=c)\n",
    "    \n",
    "    if mean:\n",
    "        ax.axhline(y.mean(), color=\"gray\", linestyle=\"--\")\n",
    "        \n",
    "    if sections:\n",
    "        for l in [ x.max() * i for i in [ 1/4, 1/2, 3/4] ]:\n",
    "            ax.axvline(l, color=\"gray\", linewidth=1, linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_melodic_profile(notelist(german_song))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can as easily plot the melodic contour of the Chinese song (we will use a different color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1, figsize=(8,6))\n",
    "\n",
    "plot_melodic_profile(notelist(german_song), ax=axes[0], mean=True)\n",
    "plot_melodic_profile(notelist(chinese_song), ax=axes[1], c=\"firebrick\", mean=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/melodic_profiles.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashed grey lines in both plots show the average MIDI pitch of the song. \n",
    "\n",
    "But still, it is quite difficult to compare them directly. They differ both with respect to their length (see the numbers on the \"Onset\" axis) and their pitches (see \"MIDI Pitch\" axis).\n",
    "\n",
    "We need to transform them in a way that makes them directly comparable. To that end, we define a new function `standardize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(notelist):\n",
    "    \"\"\"\n",
    "    Takes a notelist as input and returns a standardized version.\n",
    "    \"\"\"\n",
    "    \n",
    "    notelist[\"Rel. MIDI Pitch\"] = (notelist[\"MIDI Pitch\"] - notelist[\"MIDI Pitch\"].mean()) / notelist[\"MIDI Pitch\"].std()\n",
    "    notelist[\"Rel. Duration\"] = notelist[\"Duration\"] / notelist[\"Duration\"].sum()\n",
    "    notelist[\"Rel. Onset\"] = notelist[\"Onset\"] / notelist[\"Onset\"].max()\n",
    "    \n",
    "    return notelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize(notelist(german_song))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_melodic_profile(standardize(notelist(german_song)), mean=True, sections=True, standardized=True)\n",
    "plot_melodic_profile(standardize(notelist(chinese_song)), c=\"firebrick\", standardized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the songs makes it possible to compare them directly: They have now the same length 1 and their pitches are centered around the mean 0 with a standard deviation of 1.\n",
    "\n",
    "However, already with two pieces this plot is quite crowded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for i, song in enumerate(songs):\n",
    "    df = standardize(notelist(song))\n",
    "    df[\"Song ID\"] = i\n",
    "    dfs.append(df)\n",
    "\n",
    "big_df = pd.concat(dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.to_csv(\"data/big_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The melodic arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "grouped = big_df.groupby(\"Song ID\")\n",
    "\n",
    "for i, g in grouped:\n",
    "    x = g[\"Rel. Onset\"]\n",
    "    y = g[\"Rel. MIDI Pitch\"]\n",
    "    ax.plot(x,y, lw=.5, c=\"tab:red\", alpha=1/300)\n",
    "        \n",
    "ax.axvline(.25, lw=1, ls=\"--\", c=\"gray\")\n",
    "ax.axvline(.5, lw=1, ls=\"--\", c=\"gray\")\n",
    "ax.axvline(.75, lw=1, ls=\"--\", c=\"gray\")\n",
    "ax.axhline(0, lw=1, ls=\"--\", c=\"gray\")\n",
    "\n",
    "lowess = sm.nonparametric.lowess\n",
    "big_x = big_df[\"Rel. Onset\"]\n",
    "big_y = big_df[\"Rel. MIDI Pitch\"]\n",
    "big_z = lowess(big_y, big_x, frac=5/100, delta=1/20)\n",
    "ax.plot(big_z[:,0], big_z[:,1], c=\"black\", lw=2)\n",
    "\n",
    "plt.title(\"Melodic arc\")\n",
    "plt.xlabel(\"Relative onset\")\n",
    "plt.ylabel(\"Pitch deviation\")\n",
    "plt.xticks(np.linspace(0,1,5))\n",
    "plt.yticks(np.linspace(-5,5,11))\n",
    "plt.xlim(0,1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/melodic_arc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the melodic arc emerges as a stable shape over the entire EFC, and that sub-phrases of the songs likewise have an arc-like shape. In the remainder of this section, we look at **intervals**, the distance between two notes.\n",
    "\n",
    "Let's come back to the song *Die plappernden Junggesellen*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_song.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already extracted its notes and stored them in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df[big_df[\"Song ID\"] == 70].head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above reads as \"Select all rows in `big_df` for which the column `Song ID` is equal to 70\". The `.head()` method displays the first 5 rows by default but you can specify the number of rows you want to be displayed (here 8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing on the \"MIDI Pitch\" column, the notes in the first phrase have MIDI pitch 62, 67, 71, 74, 72. Since intervals correspond to the difference between notes, the intervals for the beginning of this song are: \n",
    "\n",
    "* +5 (67-62)\n",
    "* +4 (71-67)\n",
    "* +3 (74-71)\n",
    "* -2 (72-74)\n",
    "* -1 (71-72)\n",
    "* -2 (69-71)\n",
    "* -2 (67-69)\n",
    "\n",
    "The sequence of intervals in this phrase is thus `[+5, +4, +3, -2, -1, -2, -2]`. The signs (+ or -) also reflect the arc-like shape of this first phrase, but the sizes of the intervals are not perfecly balanced. Note that `-2` (two descending semitones, or one descending whole tone) is the most frequent interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ints = [ p2 - p1 for i, g in big_df.groupby(\"Song ID\") for p1, p2 in zip(g[\"MIDI Pitch\"], g[\"MIDI Pitch\"][1:]) ]\n",
    "min_int = min(all_ints)\n",
    "max_int = max(all_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_int, max_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_df = pd.DataFrame(0, index=np.arange(min_int,max_int), columns=np.arange(min_int,max_int+1))\n",
    "\n",
    "for i, g in big_df.groupby(\"Song ID\"):\n",
    "    intervals = [ p2 - p1 for p1, p2 in zip(g[\"MIDI Pitch\"], g[\"MIDI Pitch\"][1:])]\n",
    "        \n",
    "    for i1, i2 in zip(intervals, intervals[1:]):\n",
    "        ints_df.loc[i1,i2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(ints_df.loc[-12:13,-12:13], cmap=\"coolwarm\", square=True, linewidths=0.01,ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most common interval pairs are `(0,0)` and `(-2,-2)`. A much less frequent pair of intervals is `(5,0)`, but this is still much more frequent than, for example, `(9,9)`.\n",
    "\n",
    "To which melodic fragments do these correspond? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df[\"Avg. MIDI Pitch\"] = 0\n",
    "\n",
    "for i, group in big_df.groupby(\"Song ID\"):\n",
    "    grp_mean_pitch = int(group[\"MIDI Pitch\"].mean())\n",
    "    big_df.loc[big_df[\"Song ID\"] == i, \"Avg. MIDI Pitch\"] = grp_mean_pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df[\"shifted_pitch\"] = big_df[\"MIDI Pitch\"] - big_df[\"Avg. MIDI Pitch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(big_df[\"shifted_pitch\"].min(), big_df[\"shifted_pitch\"].max() + 1)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_df = pd.DataFrame(0, index=idx, columns=idx)\n",
    "transitions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i, group in big_df.groupby(\"Song ID\"):\n",
    "    for bg in zip(group[\"shifted_pitch\"], group[\"shifted_pitch\"][1:]):\n",
    "        transitions_df.loc[bg[0],bg[1]] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {transitions_df.sum().sum()} intervals in total in the corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "g = sns.heatmap(transitions_df, cmap=\"coolwarm\", linewidths=.01, square=True)\n",
    "plt.ylabel(\"First interval\")\n",
    "plt.xlabel(\"Second interval\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
